{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developed by Carlos Gonçalves, with funding from the São Paulo State Foundation - FAPESP, Grant 2021/01363-6\n",
    "# See README for more information\n",
    "#\n",
    "# This is the third of four scripts.\n",
    "\n",
    "# This script uses the output of previous scripts to produce a graph that represents a network derived from \n",
    "# the documentation, where nodes/vertices are people (as far as possible, disambiguated identifications) and \n",
    "# edges/links connect pairs of people that occur at least once in a same document.\n",
    "#\n",
    "# The script then produces a partition of the graph, using the Louvain algorithm for maximazing modularity\n",
    "# as implemented in the NetworkX package. If the file identities.txt is empty, the partition thus obtained \n",
    "# can be used to disambiguation, by a human analyst. Disambiguation solutions are noted in the identities.txt \n",
    "# file. For they to have effect, all scripts must be rerun (parsing, normalizing, main graph).\n",
    "#\n",
    "# Nodes carry additional information, like preferred commodity, number of the modularity class the \n",
    "# person belongs to, and some information about the documents in which the person appears.\n",
    "#\n",
    "# The resulting graph if written in graphml and gexf files.\n",
    "#\n",
    "# Input files:\n",
    "# ../Processing_output/normalized_names+run_date_out+.txt\n",
    "# Output file:\n",
    "# ../Processing_Output/subcomunidades+run_date+.txt\n",
    "# ../Processing_Output/main_graph+run_date_out+.graphml\n",
    "# ../Processing_output/main_graph+run_date_out+.gexf\n",
    "\n",
    "\n",
    "\n",
    "run_date = '_2023_12_05'\n",
    "run_date_out = '_2023_12_05'\n",
    "names_date = '_2023_03_13'\n",
    "identities_date = '_2023_03_14'\n",
    "\n",
    "base_folder = \"..\"\n",
    "\n",
    "import networkx as nx\n",
    "import io\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# This is for modularity\n",
    "import community\n",
    "h = io.open(base_folder+\"/Processing_Output/normalized_names\"+run_date+\".txt\",\"r\", encoding = \"utf-8\")\n",
    "G = nx.Graph()\n",
    "\n",
    "# declare here the list of documents to be analysed; they are numbered 1 to 157\n",
    "# 75, 116, 119, 120, 121 are documents of a different nature, so they should not be included in the SNA study\n",
    "list_of_docs = [i for i in range(1,119)]+[124] # [i for i in range(1,119) if ((i != 75) & (i != 116) & (i != 12))]\n",
    "\n",
    "# here are the docs that contain dates; I want them to be rendered squared\n",
    "dated_docs = [1,2,3,4,5,6,12,13,14,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,75,115,120,124]\n",
    "date_formulae = ['h','f','ah','z','m','d','g','z','ae','y','w','w','r','r','i','w','w','w','x','v','ka','kb','a','ad','aa','e','u','ab','q','p','+s']\n",
    "datas = {}\n",
    "\n",
    "# colors for the modularity classes\n",
    "cor = []\n",
    "for i in range(0,len(dated_docs)):\n",
    "    datas[dated_docs[i]] = date_formulae[i]\n",
    "counter = 0\n",
    "names_this_doc = []\n",
    "\n",
    "# this funcion is called after the graph is created and partitioned, and it \n",
    "# creates the hexadecimal color codes according to the number of modularity classes\n",
    "color_dict = {}\n",
    "def create_colors(set_of_numbers):\n",
    "    list_of_numbers = list(sorted(set_of_numbers))\n",
    "    number_of_colors = len(list_of_numbers)\n",
    "    step = int(255 / number_of_colors)\n",
    "    n = step\n",
    "    for number in list_of_numbers:\n",
    "        base16 = f'{n:0x}'.zfill(2)\n",
    "        color_code=\"#\"+base16+base16+base16.strip(\"x\")       \n",
    "        color_dict[number] = color_code\n",
    "        n = n + step\n",
    "\n",
    "\n",
    "# this function takes a list of names and \n",
    "# creates all the edges linking two of these names\n",
    "def create_edges(list_of_edges, number_of_doc):\n",
    "# in some damaged tablets, the name of Nur-Shamash is not readable\n",
    "# if this is the case, let us add his name\n",
    "#    print(\"Vamos criar as arestas do doc. \", number_of_doc)\n",
    "    if ('Nūr-Šamaš son of Kūbiya' not in list_of_edges):\n",
    "        list_of_edges.append('Nūr-Šamaš son of Kūbiya')\n",
    "# But it may also happen that his name is written only \n",
    "    set_of_edges = set(list_of_edges)\n",
    "    pairs_of_edges = set(itertools.combinations(set_of_edges,2))\n",
    "#    print(pairs_of_edges)\n",
    "#    G.add_edges_from(pairs_of_edges)\n",
    "    for aresta in pairs_of_edges:\n",
    "        if (aresta not in G.edges):\n",
    "            G.add_edge(*aresta)\n",
    "            G.edges[aresta]['weight'] = 1\n",
    "            G.edges[aresta]['label'] = str(number_of_doc)\n",
    "        else:\n",
    "            G.edges[aresta]['weight'] = G.edges[aresta]['weight'] + 1\n",
    "            G.edges[aresta]['label'] = G.edges[aresta]['label'] + ', ' + str(number_of_doc)\n",
    "\n",
    "            \n",
    "# now we begin!\n",
    "for line in h:\n",
    "    #print (line)\n",
    "    linha_quebrada = line.split('|')\n",
    "# now it comes the check to verify whether we are in the chosen range of docs.\n",
    "    if (int(linha_quebrada[5]) not in list_of_docs):\n",
    "        continue\n",
    "    if (int(linha_quebrada[5]) in dated_docs):\n",
    "        numero_lados_dated = 'Rectangle'\n",
    "        dated_doc = linha_quebrada[5]\n",
    "    else:\n",
    "        numero_lados_dated = 'Ellipse'\n",
    "        dated_doc = ''\n",
    "# each time material from a new document begins being parsed, a few things happen:\n",
    "# 1) the edges must be spanned\n",
    "# 2) the list of the names in this doc (now the previous doc) must be reset\n",
    "#    it is this list that will be spanned to create new edges for the graph\n",
    "# 3) counter is updated (to the value of the just accepted document)\n",
    "    if(counter != int(linha_quebrada[5])):\n",
    "        if (len(names_this_doc) != 0):\n",
    "            create_edges(names_this_doc, counter)\n",
    "        names_this_doc = []\n",
    "        counter = int(linha_quebrada[5])\n",
    "# a special case is Nur-Shamash, the lender; the name of this father must always appear    \n",
    "    if((linha_quebrada[0]== 'Nūr-Šamaš') and (linha_quebrada[4] == 'L')):\n",
    "        linha_quebrada[1] = 'son of'\n",
    "        linha_quebrada[2] = 'Kūbiya'\n",
    "# if a name is accompanied by a father's name, take the father's name into account to create a node\n",
    "# if a name is accompanied by a owner's name, then we take into account the \"slave\" satutus of the person\n",
    "# idem for wife\n",
    "    if(linha_quebrada[1] == 'son of'):\n",
    "        novo_no = linha_quebrada[0]+\" son of \"+linha_quebrada[2]\n",
    "    elif (linha_quebrada[1] == 'daughter of'):\n",
    "        novo_no = linha_quebrada[0]+\" daughter of \"+linha_quebrada[2]\n",
    "    elif (linha_quebrada[1] == 'slave of'):\n",
    "        novo_no = linha_quebrada[0]+\" slave of \"+linha_quebrada[2]\n",
    "    elif(linha_quebrada[1] == 'wife of'):\n",
    "        novo_no = linha_quebrada[0]+\" wife of \"+linha_quebrada[2]\n",
    "    else:\n",
    "        novo_no = linha_quebrada[0]\n",
    "# Let us add profession to the name of the node (if there is one)\n",
    "# Also, it is useful that profession appears as a separate node attribute and that is the \n",
    "# funcion of the variable profession\n",
    "    if (linha_quebrada[3] != ''):\n",
    "        novo_no = novo_no + ', ' + linha_quebrada[3]\n",
    "        profession = linha_quebrada[3]\n",
    "    else:\n",
    "        profession = '--'\n",
    "\n",
    "\n",
    "# is this a new or a repeated node?\n",
    "    if novo_no not in list(G.nodes):\n",
    "#        print(\"New node!!!\")\n",
    "        G.add_node(novo_no)\n",
    "        G.nodes[novo_no]['grau'] = 1\n",
    "        G.nodes[novo_no]['profissao'] = profession\n",
    "        G.nodes[novo_no]['PolygonDated'] = numero_lados_dated\n",
    "        G.nodes[novo_no]['Dated'] = dated_doc\n",
    "        G.nodes[novo_no]['Roles'] = linha_quebrada[4]\n",
    "        G.nodes[novo_no]['Appears'] = int(linha_quebrada[5])\n",
    "        G.nodes[novo_no]['Disappears'] = int(linha_quebrada[5])\n",
    "\n",
    "        \n",
    "        if(\"B\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Barley'] = 1\n",
    "        else:\n",
    "            G.nodes[novo_no]['Barley'] = 0\n",
    "        \n",
    "        if(\"S\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Silver'] = 1\n",
    "        else:\n",
    "            G.nodes[novo_no]['Silver'] = 0\n",
    "        \n",
    "        if(\"E\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Emmer'] = 1\n",
    "        else:\n",
    "            G.nodes[novo_no]['Emmer'] = 0\n",
    "            \n",
    "        if(\"C\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Chickpea'] = 1\n",
    "        else:\n",
    "            G.nodes[novo_no]['Chickpea'] = 0\n",
    "        \n",
    "        G.nodes[novo_no]['ValueBarley'] = float(linha_quebrada[9])\n",
    "        G.nodes[novo_no]['ValueSilver'] = float(linha_quebrada[10])\n",
    "        G.nodes[novo_no]['ValueEmmer'] = float(linha_quebrada[11])\n",
    "        G.nodes[novo_no]['ValueChickpea'] = float(linha_quebrada[12])\n",
    "         \n",
    "        \n",
    "        if (linha_quebrada[4] == 'B'):\n",
    "            G.nodes[novo_no]['B_ValueBarley'] = float(linha_quebrada[9])\n",
    "            G.nodes[novo_no]['B_ValueSilver'] = float(linha_quebrada[10])\n",
    "            G.nodes[novo_no]['B_ValueEmmer'] = float(linha_quebrada[11])\n",
    "            G.nodes[novo_no]['B_ValueChickpea'] = float(linha_quebrada[12])\n",
    "        else:\n",
    "            G.nodes[novo_no]['B_ValueBarley'] = float(\"0\")\n",
    "            G.nodes[novo_no]['B_ValueSilver'] = float(\"0\")\n",
    "            G.nodes[novo_no]['B_ValueEmmer'] = float(\"0\")\n",
    "            G.nodes[novo_no]['B_ValueChickpea'] = float(\"0\")\n",
    "\n",
    "        if (linha_quebrada[4] == 'W'):\n",
    "            G.nodes[novo_no]['W_ValueBarley'] = float(linha_quebrada[9])\n",
    "            G.nodes[novo_no]['W_ValueSilver'] = float(linha_quebrada[10])\n",
    "            G.nodes[novo_no]['W_ValueEmmer'] = float(linha_quebrada[11])\n",
    "            G.nodes[novo_no]['W_ValueChickpea'] = float(linha_quebrada[12])\n",
    "        else:\n",
    "            G.nodes[novo_no]['W_ValueBarley'] = float(\"0\")\n",
    "            G.nodes[novo_no]['W_ValueSilver'] = float(\"0\")\n",
    "            G.nodes[novo_no]['W_ValueEmmer'] = float(\"0\")\n",
    "            G.nodes[novo_no]['W_ValueChickpea'] = float(\"0\")\n",
    "        \n",
    "    else:\n",
    "#        print(\"Repeated node...\")\n",
    "        G.nodes[novo_no]['grau'] = G.nodes[novo_no]['grau'] + 1\n",
    "        G.nodes[novo_no]['Dated'] = G.nodes[novo_no]['Dated']+dated_doc\n",
    "        if (int(linha_quebrada[5]) < G.nodes[novo_no]['Appears']):\n",
    "            G.nodes[novo_no]['Appears'] = int(linha_quebrada[5])\n",
    "        if (int(linha_quebrada[5]) > G.nodes[novo_no]['Disappears']):\n",
    "            G.nodes[novo_no]['Disappears'] = int(linha_quebrada[5])\n",
    "\n",
    "\n",
    "        if (G.nodes[novo_no]['PolygonDated'] == 'Ellipse'):\n",
    "            G.nodes[novo_no]['PolygonDated'] = numero_lados_dated\n",
    "        if (G.nodes[novo_no]['Roles'] != linha_quebrada[4]):\n",
    "            G.nodes[novo_no]['Roles'] = 'D'\n",
    "        \n",
    "        if(\"B\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Barley'] = G.nodes[novo_no]['Barley'] + 1 \n",
    "       \n",
    "        if(\"S\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Silver'] = G.nodes[novo_no]['Silver'] + 1\n",
    "        \n",
    "        if(\"E\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Emmer'] = G.nodes[novo_no]['Emmer'] + 1\n",
    "\n",
    "        if(\"C\" in linha_quebrada[8]):\n",
    "            G.nodes[novo_no]['Chickpea'] = G.nodes[novo_no]['Chickpea'] + 1\n",
    "\n",
    "        G.nodes[novo_no]['ValueBarley'] = G.nodes[novo_no]['ValueBarley'] + float(linha_quebrada[9])\n",
    "        G.nodes[novo_no]['ValueSilver'] = G.nodes[novo_no]['ValueSilver'] + float(linha_quebrada[10])\n",
    "        G.nodes[novo_no]['ValueEmmer'] = G.nodes[novo_no]['ValueEmmer'] + float(linha_quebrada[11])\n",
    "        G.nodes[novo_no]['ValueChickpea'] = G.nodes[novo_no]['ValueChickpea'] + float(linha_quebrada[12])\n",
    "        \n",
    "        if (linha_quebrada[4] == 'B'):\n",
    "            G.nodes[novo_no]['B_ValueBarley'] = G.nodes[novo_no]['B_ValueBarley'] + float(linha_quebrada[9])\n",
    "            G.nodes[novo_no]['B_ValueSilver'] = G.nodes[novo_no]['B_ValueSilver'] + float(linha_quebrada[10])\n",
    "            G.nodes[novo_no]['B_ValueEmmer'] = G.nodes[novo_no]['B_ValueEmmer'] + float(linha_quebrada[11])\n",
    "            G.nodes[novo_no]['B_ValueChickpea'] = G.nodes[novo_no]['B_ValueChickpea'] + float(linha_quebrada[12])\n",
    "        \n",
    "        if (linha_quebrada[4] == 'W'):\n",
    "            G.nodes[novo_no]['W_ValueBarley'] = G.nodes[novo_no]['W_ValueBarley'] + float(linha_quebrada[9])\n",
    "            G.nodes[novo_no]['W_ValueSilver'] = G.nodes[novo_no]['W_ValueSilver'] + float(linha_quebrada[10])\n",
    "            G.nodes[novo_no]['W_ValueEmmer'] = G.nodes[novo_no]['W_ValueEmmer'] + float(linha_quebrada[11])\n",
    "            G.nodes[novo_no]['W_ValueChickpea'] = G.nodes[novo_no]['W_ValueChickpea'] + float(linha_quebrada[12])\n",
    "\n",
    "            \n",
    "            # now we add the just retrieved name (maybe + son of name) to the list of the names of the present doc\n",
    "    names_this_doc.append(novo_no)\n",
    "create_edges(names_this_doc, counter)\n",
    "h.close()\n",
    "# Select favourite commodity of each person\n",
    "for no in G.nodes():\n",
    "    if (G.nodes[no]['Emmer'] > G.nodes[no]['Barley'] and G.nodes[no]['Emmer'] > G.nodes[no]['Silver']):\n",
    "        G.nodes[no]['FavouriteCommodity'] = 'E'\n",
    "    elif (G.nodes[no]['Barley'] > G.nodes[no]['Emmer'] and G.nodes[no]['Barley'] > G.nodes[no]['Silver']):\n",
    "        G.nodes[no]['FavouriteCommodity'] = 'B'\n",
    "    elif (G.nodes[no]['Silver'] > G.nodes[no]['Barley'] and G.nodes[no]['Silver'] > G.nodes[no]['Emmer']):\n",
    "        G.nodes[no]['FavouriteCommodity'] = 'S'\n",
    "    else:\n",
    "        G.nodes[no]['FavouriteCommodity'] = 'N'\n",
    "\n",
    "\n",
    "# Maximise modularity\n",
    "H = nx.Graph()\n",
    "H = G.copy()\n",
    "# Here there are two possibilities: to include 'Nūr-Šamaš son of Kūbiya' in the modularity calculation\n",
    "#                                   to exclude 'Nūr-Šamaš son of Kūbiya' from it.\n",
    "# This has to be made manually by changing the variable exclude_nur_shamash\n",
    "exclude_nur_shamash = False\n",
    "if (exclude_nur_shamash):\n",
    "    H.remove_node('Nūr-Šamaš son of Kūbiya')\n",
    "part = community.best_partition(H)\n",
    "mod = community.modularity(part,H)\n",
    "# let us see how many modularity classes have been created\n",
    "set_of_class_numbers = set()\n",
    "for entry in part.keys():\n",
    "    set_of_class_numbers.add(part[entry])\n",
    "#print(set_of_class_numbers)\n",
    "\n",
    "# we have to create an equal number of colors\n",
    "create_colors(set_of_class_numbers)\n",
    "#print(color_dict)\n",
    "\n",
    "# now we have to include the color code in the graph nodes; there are two cases according to exclusion or not\n",
    "# of 'Nūr-Šamaš son of Kūbiya'\n",
    "for no in G.nodes():\n",
    "    if(no != 'Nūr-Šamaš son of Kūbiya'):\n",
    "        number = part.get(no)\n",
    "        G.nodes[no]['Color'] = color_dict[number]\n",
    "    else:\n",
    "        if(exclude_nur_shamash):\n",
    "            G.nodes[no]['Color'] = \"#000000\"\n",
    "        else:\n",
    "            number = part.get(no)\n",
    "            G.nodes[no]['Color'] = color_dict[number]\n",
    "\n",
    "# recording data on subcomunities\n",
    "j = io.open(base_folder+\"/Processing_Output/subcomunidades\"+run_date+\".txt\",\"w\", encoding = \"utf-8\")\n",
    "for pessoa in G.nodes():\n",
    "    if(pessoa != 'Nūr-Šamaš son of Kūbiya'):\n",
    "        j.write(str(part.get(pessoa))+'|'+str(pessoa)+'\\n')\n",
    "        G.nodes[pessoa]['Modularity'] = part.get(pessoa)\n",
    "    else:\n",
    "        if(exclude_nur_shamash):\n",
    "            j.write(\"-1\"+\"|\"+str(pessoa)+'\\n')\n",
    "            G.nodes[pessoa]['Modularity'] = \"-1\"\n",
    "        else:\n",
    "            j.write(str(part.get(pessoa))+'|'+str(pessoa)+'\\n')\n",
    "            G.nodes[pessoa]['Modularity'] = part.get(pessoa)\n",
    "j.close()\n",
    "\n",
    "# Let us introduce a new node attribute, that of connector\n",
    "# A node is a connector iff it has a neighbour with different colour.\n",
    "# First step. Add the attribute with value 1\n",
    "# connector will in fact contain the number of different communities a node keeps relations to\n",
    "# Now we iterate thoughout all the edges\n",
    "# For each edge, we look at the nodes and increment it by 1 iff the edge contains nodes from different communities\n",
    "for no in G.nodes():\n",
    "    G.nodes[no]['connector'] = 1\n",
    "    classes_dos_vizinhos = {part.get(no)}\n",
    "    for vizinho in nx.all_neighbors(G,no):\n",
    "        if(vizinho != 'Nūr-Šamaš son of Kūbiya'):\n",
    "            classes_dos_vizinhos = classes_dos_vizinhos | {part.get(vizinho)}\n",
    "    G.nodes[no]['connector'] = len(classes_dos_vizinhos)\n",
    "    #print(no, G.node[no]['connector'], classes_dos_vizinhos)\n",
    "\n",
    "# Agora vamos calcular betweeness centrality\n",
    "centralidade = set()\n",
    "centralidade = nx.betweenness_centrality(H)\n",
    "for no in H.nodes():\n",
    "#    G.nodes[no]['Centralidade'] = centralidade[no]\n",
    "    if(no != 'Nūr-Šamaš son of Kūbiya'):\n",
    "        G.nodes[no]['Centralidade'] = float(centralidade[no])\n",
    "    else:\n",
    "        G.nodes[no]['Centralidade'] = float(0)\n",
    "nx.write_graphml(G,base_folder+\"/Processing_Output/main_graph\"+run_date_out+\".graphml\")\n",
    "nx.write_gexf(G,base_folder+\"/Processing_Output/main_graph\"+run_date_out+\".gexf\")\n",
    "\n",
    "# Here a report that will be useful to compare graphs\n",
    "#print(\"Number of subcommunities: \",len(set_of_class_numbers))\n",
    "dF_nomes = pd.DataFrame((no,G.nodes[no]['Centralidade'],G.nodes[no]['connector'],G.nodes[no]['Roles']) for no in G.nodes())\n",
    "#dF_nomes = dF_nomes.set_index(0)\n",
    "dF_nomes.rename(columns={0:'Name', 1:'Betweenness', 2:'Number of Communities', 3:'Roles'}, inplace = True)\n",
    "#print (dF_nomes)\n",
    "dF_nomes = dF_nomes.sort_values('Betweenness',ascending = False)\n",
    "d1 = dF_nomes.head(6)\n",
    "d1 = d1.replace({'Roles': {'D': 'W and B'}})\n",
    "#print (d1)\n",
    "dF_nomes = dF_nomes.sort_values('Number of Communities',ascending = False)\n",
    "d2 = dF_nomes.head(7)\n",
    "d2 = d2.drop([0])\n",
    "d2 = d2.replace({'Roles': {'D': 'W and B'}})\n",
    "#print (d2)\n",
    "d3 = pd.merge(d1,d2)\n",
    "#print(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

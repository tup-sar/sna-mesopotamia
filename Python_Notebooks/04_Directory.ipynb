{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developed by Carlos Gonçalves, with funding from the São Paulo State Foundation - FAPESP, Grant 2021/01363-6\n",
    "# See README for more information\n",
    "#\n",
    "# This is the fourth of four scripts.\n",
    "#\n",
    "# This script outputs an alphabetically ordered list of people that occur in the documentation. \n",
    "# For each name, the script gives the number of the modularity class it belongs to and the numbers of the\n",
    "# classes it connects to.\n",
    "# \n",
    "# This is followed by details of each occurrence of the person in the documentation. If the file \n",
    "# identities.txt is not empty, then these occurrences may contain different ways of identifying one \n",
    "# individual. Note that the file identities.txt is manually produced by a human analyist. The production \n",
    "# is iterative and in each iteration, the human analyst examing the graph produced by 03_Main_Graph and\n",
    "# the directory produced by 04_Directory. As a consequence of the analysis carried out by the human \n",
    "# analyst, the solutions for ambiguities in the way the documents refer to people, are annotated in the\n",
    "# file identities.txt.\n",
    "#\n",
    "# For people that are indicated together with the name of a parent, the script also indicates if there is\n",
    "# someone elsewhere in the documentation with that parent's name. It changes the graph in the process.\n",
    "#\n",
    "# Input files:\n",
    "# ../Processing_Input/names+names_date+.txt\n",
    "# ../Processing_Output/subcomunidades+run_date+.txt\n",
    "# ../Processing_Output/parsing+run_date_out+.txt\n",
    "# ..Processing_Input/identities+identities_date+.txt\n",
    "# ../Processing_output/main_graph+run_date_out+.gexf\n",
    "\n",
    "# Output file:\n",
    "# ../Processing_Output/directory+run_date+.txt\n",
    "\n",
    "\n",
    "run_date = '_2023_12_05'\n",
    "names_date = '_2023_03_13'\n",
    "identities_date = '_2023_03_14'\n",
    "\n",
    "base_folder = \"..\"\n",
    "\n",
    "import io\n",
    "import re\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "# carregando as comunidades a que as pessoas pertencem\n",
    "# (nomes normalizados, levando em conta profissões e parentes)\n",
    "h = io.open(base_folder+\"/Processing_Output/subcomunidades\"+run_date+\".txt\",\"r\", encoding = \"utf-8\")\n",
    "subcomunidade = {}\n",
    "subcom_maxima = 0\n",
    "for line in h:\n",
    "#    line = re.sub(r'[_][0-9]*','',line)\n",
    "#    line = re.sub(r'[,].*','',line)\n",
    "    linha_quebrada = line.split('|')\n",
    "    subcomunidade[linha_quebrada[1].replace('\\n','')] = linha_quebrada[0]\n",
    "    if (subcom_maxima < int(linha_quebrada[0])):\n",
    "        subcom_maxima = int(linha_quebrada[0])\n",
    "h.close()\n",
    "#print (\"Comunidades carregadas.\")\n",
    "#print(subcom_maxima,subcomunidade)\n",
    "\n",
    "\n",
    "# É preciso em seguida criar uma lista alfabética de pessoas (que serão as pessoas do diretório)\n",
    "lista_de_pessoas = []\n",
    "for item in subcomunidade.keys():\n",
    "    lista_de_pessoas.append(item)\n",
    "lista_de_pessoas = sorted(list(set(lista_de_pessoas)))\n",
    "#print(\"Lista de pessoas criada.\")\n",
    "#print(lista_de_pessoas)\n",
    "\n",
    "# The following creates an alternative listing of persons, ordering by cluster and, secondarily, alphabetically\n",
    "#lista_de_pessoas = []\n",
    "#for n_subcom in range(subcom_maxima):\n",
    "#    \n",
    "#lista_de_pessoas.append(item)\n",
    "#lista_de_pessoas = sorted(list(set(lista_de_pessoas)))\n",
    "#print(\"Lista de pessoas criada.\")\n",
    "#print(lista_de_pessoas)\n",
    "\n",
    "# here are the docs that contain dates; I want them to be rendered squared\n",
    "dated_docs = [\"001\",\"002\",\"003\",\"004\",\"005\",\"006\",\"012\",\"013\",\"014\",\"021\",\"022\",\"023\",\"024\",\"025\",\"026\",\"027\",\"028\",\"029\",\"030\",\"031\",\"032\",\"033\",\"034\",\"035\",\"036\",\"037\",\"038\",\"075\",\"115\",\"120\",\"124\"]\n",
    "date_formulae = ['h','f','ah','z','m','d','g','z','ae','y','w','w','r','r','i','w','w','w','x','v','ka','kb','a','ad','aa','e','u','ab','q','p','+s']\n",
    "datas = {}\n",
    "for i in range(0,len(dated_docs)):\n",
    "    datas[dated_docs[i]] = date_formulae[i]\n",
    "#print (datas)\n",
    "years_in_subcommunities = [\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
    "\n",
    "\n",
    "# fazendo um glossário de nomes e nomes normalizados, junto com\n",
    "# um dicionário de anotações aos nomes\n",
    "f = io.open(base_folder+\"/Processing_Input/names\"+names_date+\".txt\",'r', encoding = \"utf-8\")\n",
    "glossario = {}\n",
    "anotacoes_aos_nomes = {}\n",
    "for line in f:\n",
    "    linha_quebrada = line.split('\\t')\n",
    "    if(linha_quebrada[1] != ''):\n",
    "        anotacoes_aos_nomes[linha_quebrada[1]] = ''\n",
    "        glossario[linha_quebrada[2]] = linha_quebrada[1]\n",
    "    else:\n",
    "        glossario[linha_quebrada[2]] = linha_quebrada[1]\n",
    "    if (\"Unknown\" in glossario[linha_quebrada[2]]):\n",
    "        #print(glossario[linha_quebrada[2]], linha_quebrada[2])\n",
    "        glossario[linha_quebrada[2]] = glossario[linha_quebrada[2]]+\"(\"+linha_quebrada[2]+\")\"\n",
    "    if(linha_quebrada[0] != linha_quebrada[1]):\n",
    "        if(linha_quebrada[0] == '('+linha_quebrada[1]+')'):\n",
    "            anotacoes_aos_nomes[linha_quebrada[1]] = \"Spelling still to be verified: \"+linha_quebrada[0]+'\\n'\n",
    "        #else:\n",
    "        #    anotacoes_aos_nomes[linha_quebrada[1]] = \"Reschid gives a different spelling: \"+linha_quebrada[0]+'\\n'\n",
    "    if(linha_quebrada[3].strip() != ''):\n",
    "        anotacoes_aos_nomes[linha_quebrada[1]] = anotacoes_aos_nomes[linha_quebrada[1]]+linha_quebrada[3]\n",
    "f.close()\n",
    "#print(\"Dicionário de normalizações de nomes carregado.\")\n",
    "\n",
    "# carregando o arquivo geral na memória\n",
    "e = io.open(base_folder+\"/Processing_Output/parsing\"+run_date+\".txt\",\"r\", encoding = \"utf-8\")\n",
    "arquivo_geral = []\n",
    "for line in e:\n",
    "    linha_quebrada = line.split(\"|&\")\n",
    "    arquivo_geral.append(linha_quebrada)\n",
    "e.close()\n",
    "#print(\"Arquivo geral carregado.\")\n",
    "\n",
    "\n",
    "# loading identities into memory\n",
    "i = io.open(base_folder+\"/Processing_Input/identities\"+identities_date+\".txt\",\"r\", encoding = \"utf-8\")\n",
    "replacements = {}\n",
    "for line in i:\n",
    "    if (line[0] == \"#\"):\n",
    "        continue\n",
    "    line = line.strip()\n",
    "    linha_quebrada = line.split('|')\n",
    "    replacements[linha_quebrada[0],linha_quebrada[1],linha_quebrada[2]] = [linha_quebrada[3],linha_quebrada[4],linha_quebrada[5],linha_quebrada[6]]\n",
    "i.close()\n",
    "#print(\"Identities loaded.\")\n",
    "#print(replacements)\n",
    "\n",
    "\n",
    "# Aqui começa o processamento. \n",
    "# Devemos percorrer toda a lista de nomes normalizados\n",
    "d = io.open(base_folder+\"/Processing_Output/directory\"+run_date+\".txt\",\"w\", encoding = \"utf-8\")\n",
    "t = io.open(base_folder+\"/Processing_Output/aliases\"+run_date+\".txt\",\"w\", encoding = \"utf-8\")\n",
    "G = nx.Graph()\n",
    "G = nx.read_gexf(base_folder+\"/Processing_Output/main_graph\"+run_date+\".gexf\")\n",
    "for no in G.nodes():\n",
    "    G.nodes[no]['Maybe_Parent'] = 0\n",
    "for item in lista_de_pessoas:\n",
    "    if (1==1): #item in subcomunidade.keys()): hahaha essa condição não é necessária\n",
    "        if(True): #subcomunidade[item] == '0' and 'son of' in item):\n",
    "            #print('\\n'+item+'\\n'+'Sub-community '+subcomunidade[item]+'.')\n",
    "            guarda_subcomunidade = '\\n'+item+'\\n'+'Sub-community '+subcomunidade[item]+'.'\n",
    "            if(\"son of\" in item):\n",
    "                item_quebrado = item.split(\" \")\n",
    "                papai = item_quebrado[len(item_quebrado)-1]\n",
    "                for possivel_pai in subcomunidade.keys():\n",
    "                    if(papai in (possivel_pai.split(\" \"))[0]):\n",
    "                        guarda_possible_parent = \"Possible parent: \"+str(possivel_pai)+\", in sub-community(ies) \"+str(subcomunidade[possivel_pai])+\".\"\n",
    "                        G.nodes[possivel_pai]['Maybe_Parent'] = G.nodes[possivel_pai]['Maybe_Parent'] + 1\n",
    "            else:\n",
    "                guarda_possible_parent = \"\"\n",
    "            #d.write('\\n'+item+' Sub-community: '+subcomunidade[item]+'\\n'+'.')\n",
    "    else:\n",
    "        #print('\\n'+item)\n",
    "        d.write('\\n'+item+'\\n')\n",
    "    if item in anotacoes_aos_nomes.keys():\n",
    "        if (anotacoes_aos_nomes[item] != ''):\n",
    "            #print((anotacoes_aos_nomes[item]).replace(\"\\n\",\"\"))\n",
    "            d.write(anotacoes_aos_nomes[item]+'\\n')\n",
    "    numero_de_ocorrencias = 0\n",
    "    conjunto = set()\n",
    "    conjunto_doc_subc = set()\n",
    "    if (item in G.nodes()):\n",
    "        for i in nx.all_neighbors(G,item):\n",
    "            if (i in subcomunidade.keys() and 'Nūr-Šamaš son of Kūbiya' != i):        \n",
    "                conjunto = conjunto | {int(subcomunidade[i])}\n",
    "                list_of_docs = G.edges[item,i]['label'].split(', ')\n",
    "                for element in list_of_docs:\n",
    "                    element = element.zfill(3)\n",
    "                    conjunto_doc_subc = conjunto_doc_subc | {(element,subcomunidade[i])}\n",
    "        to_clusters_1 = list(sorted(conjunto))\n",
    "        to_clusters_1 = [str(x) for x in (to_clusters_1)]\n",
    "        to_clusters_1 = ' '.join(to_clusters_1).replace(' ',', ')\n",
    "        #print(guarda_subcomunidade, \"Connects to clusters \"+to_clusters_1+\".\")\n",
    "        d.write(guarda_subcomunidade+\" Connects to clusters \"+to_clusters_1+\".\\n\")\n",
    "        if (guarda_possible_parent !=\"\"):\n",
    "            #print(guarda_possible_parent)\n",
    "            guarda_possible_parent = \"\"\n",
    "        #print(\"Connections via Documents\")\n",
    "        #print (\"Documents     Subcommunities\")        \n",
    "        #for element in sorted(conjunto_doc_subc):\n",
    "        #    print(element[0]+\"            \"+element[1])\n",
    "# here comes the most important part of the directory. We check for all occorências transliterations in \n",
    "# arquivo_geral that correspond to the item in lista_de_pessoas being processed at this moment\n",
    "# The ocorrências must be built from name, kinship, relative and profession (if it all exist), producing the\n",
    "# termos de comparação\n",
    "    for ocorrencia in arquivo_geral:  \n",
    "# The first thing to do is to verify whether ocorrencia in the arquivo_geral \n",
    "# corresponds to a homonym or an alias. If this is the case, the present item will have to be compared with the \n",
    "# replacement of ocorrencia in the dicionary of replacement. If that is not the case, the present item\n",
    "# will be compared with the ocorrencia in arquivo geral\n",
    "#\n",
    "# Thus, in order to veryfy if this is the case, \n",
    "# we must check whether ocorrencia is listed in the dictionary of replacements\n",
    "        nominho = (ocorrencia[0], ocorrencia[5], ocorrencia[6]+ocorrencia[7])\n",
    "        if (nominho in replacements.keys()):\n",
    "            replacement_case = True\n",
    "        else:\n",
    "            replacement_case = False\n",
    "# Now, suppose that in fact the present ocorrencia is in the dicionary of replacements. It needs therefore to\n",
    "# be replaced\n",
    "        if(replacement_case == True):            \n",
    "            replacing_name = replacements[nominho]\n",
    "            termo_de_comparacao = replacing_name[0]\n",
    "            para_imprimir = glossario[ocorrencia[0]]+\" (\"+ocorrencia[0]+\")\"\n",
    "            if(replacing_name[1] !=''):  # so, there is a kinship and a relative \n",
    "                termo_de_comparacao = termo_de_comparacao + \" \" +replacing_name[1] + \" \" + replacing_name[2]\n",
    "                if(ocorrencia[1] != ''):\n",
    "                    para_imprimir = para_imprimir+\" \"+ocorrencia[1]+ \" \" + glossario[ocorrencia[2]]+\" (\"+ocorrencia[2]+\")\"\n",
    "            if(replacing_name[3] !=''): # so, there is a profession\n",
    "                termo_de_comparacao = termo_de_comparacao + \", \" + replacing_name[3]\n",
    "                if(ocorrencia[3] != ''):\n",
    "                    para_imprimir = para_imprimir+\", \"+ocorrencia[3]\n",
    "        else:  # that is to say, if replacement_case is False\n",
    "            termo_de_comparacao = glossario[ocorrencia[0]]\n",
    "            if (\"Unknown\" in termo_de_comparacao):\n",
    "                termo_de_comparacao = termo_de_comparacao+\"(\"+ocorrencia[0]+\")\"\n",
    "            para_imprimir = glossario[ocorrencia[0]]+\" (\"+ocorrencia[0]+\")\"\n",
    "            if(ocorrencia[1] != ''):\n",
    "                termo_de_comparacao = termo_de_comparacao+\" \"+ocorrencia[1]+ \" \" + glossario[ocorrencia[2]]\n",
    "                para_imprimir = para_imprimir+\" \"+ocorrencia[1]+ \" \" + glossario[ocorrencia[2]]+\" (\"+ocorrencia[2]+\")\"\n",
    "            if(ocorrencia[3] != ''):\n",
    "                termo_de_comparacao = termo_de_comparacao+\", \"+ocorrencia[3]\n",
    "                para_imprimir = para_imprimir+\", \"+ocorrencia[3]\n",
    "\n",
    "# once the termo_de_comparacao is built, we can finally check whether it is the item being \n",
    "# analysed at this iteration\n",
    "        lista_comunidades_do_doc = \"\"\n",
    "        if(item == termo_de_comparacao):\n",
    "            #print(item)\n",
    "            for element in sorted(conjunto_doc_subc):\n",
    "                if ocorrencia [5] in element[0]:\n",
    "                    lista_comunidades_do_doc = lista_comunidades_do_doc+\" \"+element[1]\n",
    "            #print(lista_comunidades_do_doc)\n",
    "            lista_comunidades_do_doc = (lista_comunidades_do_doc.lstrip()).split(' ')\n",
    "            to_clusters = [int(x) for x in lista_comunidades_do_doc if x !='']\n",
    "            to_clusters = [str(x) for x in sorted(to_clusters)]\n",
    "            to_clusters = ' '.join(to_clusters).replace(' ',', ')\n",
    "            if (ocorrencia[5] in datas.keys()):\n",
    "                #print(para_imprimir+\", \"+ocorrencia[4]+\", \"+ocorrencia[5]+\", \"+ocorrencia[6]+ocorrencia[7]+\", to cluster(s) \"+to_clusters+\". Year name:\"+datas[ocorrencia[5]])\n",
    "                if (years_in_subcommunities[int(subcomunidade[item])] == \"\"):\n",
    "                    years_in_subcommunities[int(subcomunidade[item])] = datas[ocorrencia[5]]+ocorrencia[5]\n",
    "                else:\n",
    "                    if(datas[ocorrencia[5]]+ocorrencia[5] not in years_in_subcommunities[int(subcomunidade[item])]):\n",
    "                        years_in_subcommunities[int(subcomunidade[item])] = years_in_subcommunities[int(subcomunidade[item])]+\" ,\"+datas[ocorrencia[5]]+ocorrencia[5]\n",
    "                        \n",
    "            else:\n",
    "                #print(para_imprimir+\", \"+ocorrencia[4]+\", \"+ocorrencia[5]+\", \"+ocorrencia[6]+ocorrencia[7]+\", to cluster(s) \"+to_clusters)\n",
    "                d.write(para_imprimir+\", \"+ocorrencia[4]+\", \"+ocorrencia[5]+\", \"+ocorrencia[6]+ocorrencia[7]+\", to cluster(s) \"+to_clusters+\"\\n\")\n",
    "            numero_de_ocorrencias = numero_de_ocorrencias + 1\n",
    "    t.write(item+\"|\"+subcomunidade[item]+\"|\"+(\", \".join(str(sorted(conjunto))))+\"|\"+str(numero_de_ocorrencias)+\"\\n\")\n",
    "d.close()\n",
    "t.close()\n",
    "nx.write_graphml(G,base_folder+\"/Processing_Output/main_graph\"+run_date+\".graphml\")\n",
    "nx.write_gexf(G,base_folder+\"/Processing_Output/main_graph\"+run_date+\".gexf\")\n",
    "#print(\"Total de clusters:\",1+ subcom_maxima)\n",
    "#for i in range(len(years_in_subcommunities)):\n",
    "#    print(\"Subcommunity \"+str(i)+\": \"+years_in_subcommunities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
